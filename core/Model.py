import urllib
import urllib.error
import warnings
from enum import Enum
from typing import Optional
from tqdm import tqdm
import openai
import ollama


class SupportedModels:
    class OpenAI(Enum):
        ''' Supported OpenAI models'''
        GPT4o = 'gpt-4o'
        GPT4o_mini = 'gpt-4o-mini'
    class Ollama(Enum):
        ''' Supported Ollama models'''
        llama_31_8B = 'llama3.1:8b'
        llama_32_1b = 'llama3.2:1b'
        llama_32_3b = 'llama3.2:3b'


class GenerationConfig:
    def __init__(
        self, 
        temperature: float = 0.7, 
        top_p: float = 1, 
        max_tokens: int = 2048, 
        num_ctx: int = 4096*8, 
        seed: int = 42
    ):
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.num_ctx = num_ctx
        self.seed = seed


class LLM:
    """ Text generation model handler. Supports OpenAI and Ollama models. """
    
    def __init__(
        self, model: SupportedModels.Ollama | SupportedModels.OpenAI, is_debug: bool = False,
        api_key: Optional[str] = None, base_url: Optional[str] = None, client_timeout: float = 300.0,
    ):
        """ Attributes
            ----------
                model: SupportedModels.Ollama | SupportedModels.OpenAI
                    Name of text generation model, hosted locally or on the cloud.
                client: openai.OpenAI | ollama.Client 
                    Client for communicating with OpenAI or Ollama models. 
                    Initialised using:
                        base_url: str
                            base url for ollama model; defaults to 'http://localhost:11434/' if None provided
                        api_key: str
                            valid openai api key; ignored if model is SupportedModel.Ollama
                __text_generator_fn: Callable[[Sequence[Messages], GenerationConfig], str]
                    Handles text generation using OpenAI or Ollama API
                is_debug: bool
                    Debug mode if True; prints prompt instead of messaging model via API client
                n_input_tokens: list[int]
                    Number of prompt tokens sent in each request
                n_output_tokens: list[int]
                    Number of completion tokens generated by the model for each request
                CLIENT_TIMEOUT: float
                    Timeout value in seconds for client
        """
        self.model = model
        self.n_input_tokens:  list[int] = []
        self.n_output_tokens: list[int] = []
        self.CLIENT_TIMEOUT = client_timeout
        self.is_debug = self.set_debug(is_debug)
        self.client, self.__text_generator_fn = self.__setup_client(base_url, api_key)

    def __call__(self, messages: list[dict[str, str]], cfg: GenerationConfig) -> str:
        """ Messages model using OpenAI/Ollama client and returns the text generated. Prints prompt in debug mode. """
        if self.is_debug:
            reply = self.__generate_debug(messages)
        else:
            reply, prompt_tokens, completion_tokens = self.__text_generator_fn(messages, cfg)          
            self.n_input_tokens.append(prompt_tokens)
            self.n_output_tokens.append(completion_tokens)
        return reply
    
    def __generate_openai(self, messages: list[dict[str, str]], cfg: GenerationConfig) -> tuple[str, int, int]:
        """ Generates text using OpenAI API str"""
        api_response = self.client.chat.completions.create(
            model = self.model.value,
            messages = messages, 
            temperature = cfg.temperature,
            top_p = cfg.top_p,
            max_tokens = cfg.max_tokens,
        )
        reply = api_response.choices[0].message.content
        prompt_tokens = api_response.usage.prompt_tokens
        completion_tokens = api_response.usage.completion_tokens
        return reply, prompt_tokens, completion_tokens

    def __generate_ollama(self, messages: list[dict[str, str]], cfg: GenerationConfig) -> tuple[str, int, int]:
        """ Generates text using Ollama API """
        api_response = self.client.chat(
            model = self.model.value,
            messages = messages,
            options = {
                'temperature': cfg.temperature,
                'top_p': cfg.top_p,
                'num_predict': cfg.max_tokens,
                'num_ctx': cfg.num_ctx,
                'seed': cfg.seed,
            }
        )
        reply = api_response['message']['content']
        prompt_tokens = api_response['prompt_eval_count']
        completion_tokens = api_response['eval_count']
        return reply, prompt_tokens, completion_tokens

    def __generate_debug(self, messages: list[dict[str, str]], cfg: GenerationConfig) -> str:
        separator = f"\n{'=' * 25}\n\n"
        message_str = (
            separator.join(
                f"{message['role'].upper()}:\n{message['content']}"
                for message in messages
            )
            + separator 
            + "<<MODEL RESPONSE>>\n\n"
        )
        return message_str
    

    def __pull_ollama_model(client: ollama.Client, model: SupportedModels.Ollama) -> None:
        """ Pulls Ollama model for use with LLM. """
        if model not in SupportedModels.Ollama:
            # should never execute
            raise ValueError(f"Ollama pull() Aborted: {model.value} not in SupportedModels.Ollama.")
        else:
            # Taken from https://github.com/ollama/ollama-python/blob/main/examples/pull.py
            print(f"Pulling ollama model {model.value}...")
            current_digest, bars = '', {}
            for progress in client.pull(model.value, stream=True):
                digest = progress.get('digest', '')
                if digest != current_digest and current_digest in bars:
                    bars[current_digest].close()

                if not digest:
                    print(progress.get('status'))
                    continue

                if digest not in bars and (total := progress.get('total')):
                    bars[digest] = tqdm(total=total, desc=f'pulling {digest[7:19]}', unit='B', unit_scale=True)

                if completed := progress.get('completed'):
                    bars[digest].update(completed - bars[digest].n)

                current_digest = digest
            print(f"Successfully pulled model {model.value}")

    def __setup_client(self, base_url: str, api_key: str) -> openai.OpenAI | ollama.Client:
        """ Returns a client and text_generation handler function for use with ChatGPT or locally hosted Ollama model. """
        # TODO: add something for ReadTimeout if client fails to connect
        if self.model in SupportedModels.Ollama:
            OLLAMA_BASE_URL = base_url or 'http://localhost:11434/'
            client = ollama.Client(host=OLLAMA_BASE_URL, timeout=self.CLIENT_TIMEOUT)
            text_generator_fn = self.__generate_ollama

            try:
                status_code = urllib.request.urlopen(OLLAMA_BASE_URL, timeout=5).getcode()
                LLM.__pull_ollama_model(client, self.model)
                print(f"Client Instantiated: Ollama model {self.model.value} active at {OLLAMA_BASE_URL} ({status_code=})")
            except ollama.ResponseError as e:
                raise ValueError(f"Ollama pull() Failed: ({e.__class__.__name__} {e}).")
            except urllib.error.URLError as e:
                raise ValueError(f"Ensure Ollama is running before instantiating LLM ({e.__class__.__name__} {e}).")
            except Exception as e:
                raise ValueError(f"Ollama connection error: ({e.__class__.__name__} {e}).")
            
        elif self.model in SupportedModels.OpenAI:
            client = openai.OpenAI(api_key=api_key, timeout=self.CLIENT_TIMEOUT)
            text_generator_fn = self.__generate_openai;
            
            try:
                client.models.list()
                print("Client Instantiated: OpenAI API key valid.")
            except openai.AuthenticationError:
                raise ValueError("Please provide a valid OpenAI API key.")
            
        else:
            raise ValueError("{self.model} not supported.")
        
        return client, text_generator_fn
        

    def calculate_total_generation_cost(self, input_token_cost: float, output_token_cost: float) -> None:
        """ Takes token costs in $/1M and prints the total cost of generation for this LLM's lifetime """
        cost_input  = sum(self.n_input_tokens) * (input_token_cost / 1e6)
        cost_output = sum(self.n_output_tokens) * (output_token_cost / 1e6)
        print(f"{cost_input=: .2f} | {cost_output=: .2f} | Total = ${cost_input + cost_output}")

    def count_total_tokens(self):
        print(f"Total input tokens: {sum(self.n_input_tokens)} | Total output tokens: {sum(self.n_output_tokens)}")


    def __str__(self):
        return self.model.value

    def set_debug(self, is_debug: bool):
        self.is_debug = is_debug
        if is_debug:
            warnings.warn("Debug mode ON: LLM will return prompt instead of response.")
        else:
            warnings.warn("Debug mode OFF: LLM will generate response")
        return is_debug



if __name__ == '__main__':
    llm = LLM(SupportedModels.Ollama.llama_32_1b)
    cfg = GenerationConfig(temperature= 0.7, top_p=1, max_tokens=24, num_ctx=4096*0.5, seed=42)
    text = llm(
        messages=[
            {'role': 'system', 'content': 'You are a greedy, unhelpful pirate'},
            {'role': 'user', 'content': 'Hi'}
        ],
        cfg=cfg
    )
    print(text)