{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from agents.EvaluatorForBIRD import EvaluatorForBIRD\n",
    "from agents.ZeroShotAgent import ZeroShotAgent\n",
    "from agents.OptimizerAgent import OptimizerAgent\n",
    "from agents.MultiAgentDiscussion import MultiAgentDiscussion\n",
    "\n",
    "from config import MODEL, METHOD, OUTPUT_PATH\n",
    "from utility import read_dataset, get_db_cursor, fetch_BIRD_schemas, get_openai_client, dump_to_json\n",
    "import api_keys\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_keys.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_names=['formula_1', 'debit_card_specializing', 'thrombosis_prediction'], len(df)=146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahim/Documents/sql-gen/agents/TextToSQL.py:47: UserWarning: Debug mode active- agent will only print prompts instead of generating response.\n",
      "  warnings.warn(\"Debug mode active- agent will only print prompts instead of generating response.\")\n"
     ]
    }
   ],
   "source": [
    "df, db_names = read_dataset()\n",
    "db_schemas   = fetch_BIRD_schemas(db_names)\n",
    "print(f'{db_names=}, {len(df)=}')\n",
    "\n",
    "client = get_openai_client()\n",
    "multi_agent = MultiAgentDiscussion(MODEL, client, get_db_cursor, db_schemas, OUTPUT_PATH, is_debug=True)\n",
    "\n",
    "# multi_agent.batched_generate(df[df.question_id == 944])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE customers\n",
      "(\n",
      "    CustomerID INTEGER UNIQUE     not null\n",
      "        primary key,\n",
      "    Segment    TEXT null,\n",
      "    Currency   TEXT null\n",
      ")\n",
      "CREATE TABLE gasstations\n",
      "(\n",
      "    GasStationID INTEGER    UNIQUE   not null\n",
      "        primary key,\n",
      "    ChainID      INTEGER          null,\n",
      "    Country      TEXT null,\n",
      "    Segment      TEXT null\n",
      ")\n",
      "CREATE TABLE products\n",
      "(\n",
      "    ProductID   INTEGER   UNIQUE      not null\n",
      "        primary key,\n",
      "    Description TEXT null\n",
      ")\n",
      "CREATE TABLE \"transactions_1k\"\n",
      "(\n",
      "    TransactionID INTEGER\n",
      "        primary key autoincrement,\n",
      "    Date          DATE,\n",
      "    Time          TEXT,\n",
      "    CustomerID    INTEGER,\n",
      "    CardID        INTEGER,\n",
      "    GasStationID  INTEGER,\n",
      "    ProductID     INTEGER,\n",
      "    Amount        INTEGER,\n",
      "    Price         REAL\n",
      ")\n",
      "CREATE TABLE \"yearmonth\"\n",
      "(\n",
      "    CustomerID  INTEGER not null\n",
      "        references customers\n",
      "            on update cascade on delete cascade\n",
      "        references customers,\n",
      "    Date        TEXT    not null,\n",
      "    Consumption REAL,\n",
      "    primary key (Date, CustomerID)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(db_schemas['debit_card_specializing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METHOD == 'zero-shot':\n",
    "    print(f\"Experiment: {MODEL}_{METHOD}\")\n",
    "    \n",
    "    # Setup\n",
    "    df, db_names = read_dataset()\n",
    "    db_schemas   = fetch_BIRD_schemas(db_names)\n",
    "    print(f'{db_names=}, {len(df)=}')\n",
    "    \n",
    "    client = get_openai_client()\n",
    "    agent = ZeroShotAgent(MODEL, client, get_db_cursor, db_schemas, OUTPUT_PATH)\n",
    "    evaluator = EvaluatorForBIRD(get_db_cursor)\n",
    "    \n",
    "    # Generate\n",
    "    raw_responses = agent.batched_generate(df)\n",
    "    dump_to_json('raw_responses', raw_responses)\n",
    "\n",
    "    # Parse\n",
    "    print(\"Finished Generating. Attempting SQL auto-parsing...\")\n",
    "    cleaned_sql = agent.auto_parse_sql_from_response(raw_responses)\n",
    "    dump_to_json('cleaned_sql', cleaned_sql)\n",
    "    print(\"SQL auto-parsing successful\")\n",
    "\n",
    "    # Evaluate\n",
    "    df['prediction'] = cleaned_sql\n",
    "    df['label'] = evaluator.evaluate(df, pred_col_name='prediction')\n",
    "    \n",
    "    # Save results\n",
    "    df.to_json(OUTPUT_PATH / f'{MODEL}_{METHOD}_df.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1150: OperationalError no such function: YEAR\n",
      "Q_1156: OperationalError no such function: YEAR\n",
      "Q_1162: OperationalError no such function: YEAR\n",
      "Q_1164: OperationalError no such function: YEAR\n",
      "Q_1168: OperationalError no such function: YEAR\n",
      "Q_1171: OperationalError no such function: YEAR\n",
      "Q_1175: OperationalError no such function: YEAR\n",
      "Q_1195: OperationalError no such column: L.LAB\n",
      "Q_1201: OperationalError no such function: YEAR\n",
      "Q_1227: OperationalError no such function: YEAR\n",
      "Q_1229: OperationalError no such function: YEAR\n",
      "Q_1231: OperationalError no such function: YEAR\n",
      "Q_1232: OperationalError no such function: YEAR\n",
      "Q_1235: OperationalError no such function: YEAR\n",
      "Q_1239: OperationalError no such function: YEAR\n",
      "Q_1242: OperationalError no such function: YEAR\n",
      "Q_1243: OperationalError no such function: YEAR\n",
      "Q_1254: OperationalError no such function: YEAR\n",
      "Q_1257: OperationalError no such function: YEAR\n",
      "Q_955: OperationalError no such column: d.driverId\n",
      "Q_967: Warning You can only execute one statement at a time.\n",
      "\n",
      "=== EX Results ===\n",
      "Accuracy :  35.616%\n",
      "Breakdown by Difficulty:\n",
      "\tsimple:  59.184% (29 / 49)\n",
      "\tmoderate:  30.769% (20 / 65)\n",
      "\tchallenging:  9.375% (3 / 32)\n",
      "=== end ===\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o Zero-shot + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SQL: gpt-4o_optimizer-agent\n",
      "Finished Generating\n",
      "SQL auto-parsing successful\n"
     ]
    }
   ],
   "source": [
    "if METHOD == 'optimizer-agent':\n",
    "    print(f\"Experiment: {MODEL}_{METHOD}\")\n",
    "    \n",
    "    # Setup\n",
    "    df, db_names = read_dataset()\n",
    "    db_schemas   = fetch_BIRD_schemas(db_names)\n",
    "    print(f'{db_names=}, {len(df)=}')\n",
    "    \n",
    "    client = get_openai_client()\n",
    "    agent = OptimizerAgent(MODEL, client, get_db_cursor, db_schemas, OUTPUT_PATH)\n",
    "    evaluator = EvaluatorForBIRD(get_db_cursor)\n",
    "    \n",
    "    # Generate\n",
    "    df = pd.read_json('gpt-4o_zero-shot_df.json')\n",
    "    raw_responses = agent.batched_generate(df)\n",
    "    dump_to_json('raw_responses', raw_responses)\n",
    "\n",
    "    # Parse\n",
    "    print(f\"Finished Generating. Attempting SQL auto-parsing...\")\n",
    "    cleaned_sql = agent.auto_parse_sql_from_response(raw_responses)\n",
    "    dump_to_json('cleaned_sql', cleaned_sql)\n",
    "    print(f\"SQL auto-parsing successful\")\n",
    "\n",
    "    # Evaluate\n",
    "    df['optimized'] = cleaned_sql\n",
    "    df['opt-label'] = evaluator.evaluate(df, pred_col_name='optimized')\n",
    "    \n",
    "    # Save results\n",
    "    df.to_json(OUTPUT_PATH / f'{MODEL}_{METHOD}_df.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_944: OperationalError incomplete input\n",
      "\n",
      "=== EX Results ===\n",
      "Accuracy :  45.890%\n",
      "Breakdown by Difficulty:\n",
      "\tsimple:  61.224% (30 / 49)\n",
      "\tmoderate:  43.077% (28 / 65)\n",
      "\tchallenging:  28.125% (9 / 32)\n",
      "=== end ===\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o Multi-Agent Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: gpt-4o_discussion\n",
      "db_names=['formula_1', 'debit_card_specializing', 'thrombosis_prediction'], len(df)=146\n",
      "hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL:   0%|          | 0/146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL: 100%|██████████| 146/146 [2:46:51<00:00, 68.57s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Generating. Attempting SQL auto-parse...\n",
      "SQL auto-parsing successful\n",
      "\n",
      "\n",
      "Evaluating Zero-shot starter generated queries\n",
      "--- Evaluating Performance ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:   3%|▎         | 5/146 [00:01<00:42,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1480: OperationalError incomplete input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:   5%|▍         | 7/146 [00:01<00:43,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1482: OperationalError ambiguous column name: CustomerID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:  47%|████▋     | 68/146 [00:03<00:01, 71.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1227: OperationalError no such function: YEAR\n",
      "Q_1231: OperationalError no such function: YEAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:  84%|████████▎ | 122/146 [00:04<00:00, 48.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_955: OperationalError no such column: ds.year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:  90%|█████████ | 132/146 [00:04<00:00, 43.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_967: Warning You can only execute one statement at a time.\n",
      "Q_972: OperationalError no such function: year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL: 100%|██████████| 146/146 [00:05<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EX Results ===\n",
      "Accuracy :  39.726%\n",
      "Breakdown by Difficulty:\n",
      "\tsimple:  46.939% (23 of 49)\n",
      "\tmoderate:  44.615% (29 of 65)\n",
      "\tchallenging:  18.750% (6 of 32)\n",
      "=== end ===\n",
      "\n",
      "Evaluating meta-prompt starter generated queries\n",
      "--- Evaluating Performance ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:  45%|████▌     | 66/146 [00:03<00:00, 80.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1153: Warning You can only execute one statement at a time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:  86%|████████▌ | 125/146 [00:04<00:00, 72.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_955: OperationalError no such column: ds.year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL: 100%|██████████| 146/146 [00:05<00:00, 25.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EX Results ===\n",
      "Accuracy :  48.630%\n",
      "Breakdown by Difficulty:\n",
      "\tsimple:  61.224% (30 of 49)\n",
      "\tmoderate:  47.692% (31 of 65)\n",
      "\tchallenging:  31.250% (10 of 32)\n",
      "=== end ===\n",
      "\n",
      "Evaluating Multi-Agent Discussion generated queries\n",
      "--- Evaluating Performance ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:   3%|▎         | 5/146 [00:01<00:40,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1480: OperationalError incomplete input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL:   8%|▊         | 12/146 [00:01<00:15,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1482: OperationalError ORDER BY clause should come after UNION ALL not before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing SQL: 100%|██████████| 146/146 [00:19<00:00,  7.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EX Results ===\n",
      "Accuracy :  45.890%\n",
      "Breakdown by Difficulty:\n",
      "\tsimple:  55.102% (27 of 49)\n",
      "\tmoderate:  43.077% (28 of 65)\n",
      "\tchallenging:  37.500% (12 of 32)\n",
      "=== end ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if METHOD == 'discussion':\n",
    "    print(f\"Experiment: {MODEL}_{METHOD}\")\n",
    "    \n",
    "    # Setup\n",
    "    df, db_names = read_dataset()\n",
    "    db_schemas   = fetch_BIRD_schemas(db_names)\n",
    "    print(f'{db_names=}, {len(df)=}')\n",
    "\n",
    "    client = get_openai_client()\n",
    "    multi_agent = MultiAgentDiscussion(MODEL, client, get_db_cursor, db_schemas, OUTPUT_PATH)\n",
    "    evaluator = EvaluatorForBIRD(get_db_cursor)\n",
    "\n",
    "\n",
    "    # Generate\n",
    "    raw_responses = multi_agent.batched_generate(df, rounds=3)\n",
    "    dump_to_json('raw_responses', raw_responses)\n",
    "\n",
    "    # Parse\n",
    "    print(f\"Finished Generating. Attempting SQL auto-parse...\")\n",
    "\n",
    "    starter_zero = multi_agent.auto_parse_sql_from_response([response['agent_zero_shot'][0] for response in raw_responses])\n",
    "    dump_to_json('cleaned_zeroshot_starter', starter_zero)\n",
    "\n",
    "    starter_meta = multi_agent.auto_parse_sql_from_response([response['agent_meta_prompt'][0] for response in raw_responses])\n",
    "    dump_to_json('cleaned_starter_meta', starter_meta)\n",
    "    \n",
    "    cleaned_sql  = multi_agent.auto_parse_sql_from_response([response['verdict'] for response in raw_responses])\n",
    "    dump_to_json('cleaned_sql', cleaned_sql)\n",
    "\n",
    "    print(f\"SQL auto-parsing successful\\n\\n\")\n",
    "\n",
    "\n",
    "    # Evaluate results\n",
    "    print(\"Evaluating Zero-shot starter generated queries\")\n",
    "    df['starter_zero_shot'] = starter_zero\n",
    "    df['zero_shot_labels']  = evaluator.evaluate(df, pred_col_name='starter_zero_shot')\n",
    "\n",
    "    print(\"Evaluating meta-prompt starter generated queries\")\n",
    "    df['starter_meta_prompt'] = starter_meta\n",
    "    df['meta_prompt_labels']  = evaluator.evaluate(df, pred_col_name='starter_meta_prompt')\n",
    "\n",
    "    print(\"Evaluating Multi-Agent Discussion generated queries\")\n",
    "    df['prediction'] = cleaned_sql\n",
    "    df['label']      = evaluator.evaluate(df, pred_col_name='prediction')\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    df.to_json(OUTPUT_PATH / f'{MODEL}_{METHOD}_df.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments:\n",
    "- Zero Shot\n",
    "    - with/without COT\n",
    "- Optimizer (on top of zero-shot)\n",
    "- Multi-agent:\n",
    "    - Zero-shot -> Optimizer -> Multi-agent Debate\n",
    "    - Zero-shot -> Optimizer -> Multi-agent Discussion\n",
    "    - Best of the above -> Optimizer\n",
    "- Decomposition and Generation via Multi-agent Debate/Discussion\n",
    "- Sparse Topology Multi-agent Debate/Discussion\n",
    "- Augmenting schema with LLM calls:\n",
    "    - Point out relationships (graph idea)\n",
    "    - Write short descriptions regarding tables, columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
