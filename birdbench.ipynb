{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20132389",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-24T09:16:06.536877Z",
     "iopub.status.busy": "2025-02-24T09:16:06.536577Z",
     "iopub.status.idle": "2025-02-24T09:19:22.646969Z",
     "shell.execute_reply": "2025-02-24T09:19:22.645866Z"
    },
    "papermill": {
     "duration": 196.12232,
     "end_time": "2025-02-24T09:19:22.648500",
     "exception": false,
     "start_time": "2025-02-24T09:16:06.526180",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu121\r\n",
      "Uninstalling torch-2.5.1+cu121:\r\n",
      "  Successfully uninstalled torch-2.5.1+cu121\r\n",
      "Found existing installation: pynvml 11.4.1\r\n",
      "Uninstalling pynvml-11.4.1:\r\n",
      "  Successfully uninstalled pynvml-11.4.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting vllm\r\n",
      "  Downloading vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting func_timeout\r\n",
      "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.26.4)\r\n",
      "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.60.0)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm) (4.67.1)\r\n",
      "Collecting blake3 (from vllm)\r\n",
      "  Downloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\r\n",
      "Collecting transformers>=4.48.2 (from vllm)\r\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm) (3.20.3)\r\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm) (3.11.10)\r\n",
      "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.57.4)\r\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.10.3)\r\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.1)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm) (11.0.0)\r\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\r\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\r\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm)\r\n",
      "  Downloading lm_format_enforcer-0.10.10-py3-none-any.whl.metadata (17 kB)\r\n",
      "Collecting outlines==0.1.11 (from vllm)\r\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\r\n",
      "Collecting lark==1.2.2 (from vllm)\r\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting xgrammar==0.1.11 (from vllm)\r\n",
      "  Downloading xgrammar-0.1.11-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.12.2)\r\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.16.1)\r\n",
      "Collecting partial-json-parser (from vllm)\r\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm) (24.0.1)\r\n",
      "Collecting msgspec (from vllm)\r\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\r\n",
      "Collecting gguf==0.10.0 (from vllm)\r\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (from vllm) (8.5.0)\r\n",
      "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm)\r\n",
      "  Downloading mistral_common-1.5.3-py3-none-any.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm) (6.0.2)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\r\n",
      "Collecting compressed-tensors==0.9.1 (from vllm)\r\n",
      "  Downloading compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting depyf==0.18.0 (from vllm)\r\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from vllm) (3.1.0)\r\n",
      "Requirement already satisfied: ray==2.40.0 in /usr/local/lib/python3.10/dist-packages (from ray[adag]==2.40.0->vllm) (2.40.0)\r\n",
      "Collecting torch==2.5.1 (from vllm)\r\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\r\n",
      "Requirement already satisfied: torchaudio==2.5.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.1+cu121)\r\n",
      "Collecting xformers==0.0.28.post3 (from vllm)\r\n",
      "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\r\n",
      "Collecting astor (from depyf==0.18.0->vllm)\r\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.60.0->vllm) (0.43.0)\r\n",
      "Collecting interegular (from outlines==0.1.11->vllm)\r\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (3.1.4)\r\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\r\n",
      "Collecting diskcache (from outlines==0.1.11->vllm)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (0.35.1)\r\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\r\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\r\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm)\r\n",
      "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\r\n",
      "  Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm) (8.1.7)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm) (1.1.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm) (24.2)\r\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm) (1.3.2)\r\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm) (1.5.0)\r\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (from ray[adag]==2.40.0->vllm) (12.2.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (3.4.2)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (2024.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->vllm)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->vllm)\r\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (1.13.1)\r\n",
      "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (from xgrammar==0.1.11->vllm) (2.13.6)\r\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from xgrammar==0.1.11->vllm) (8.3.4)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm) (1.3.0)\r\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.28.1)\r\n",
      "Collecting jinja2 (from outlines==0.1.11->vllm)\r\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\r\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.10.0.84)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm) (2.4.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (3.7.1)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (0.8.2)\r\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (2.27.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2024.12.14)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm) (0.27.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.48.2->vllm) (0.4.5)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (2.4.4)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (4.0.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (24.3.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (1.18.3)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata->vllm) (3.21.0)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.52.0->vllm) (1.2.2)\r\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (2.7.0)\r\n",
      "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.15.1)\r\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.14.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.22.3)\r\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\r\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (14.1)\r\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x->ray[adag]==2.40.0->vllm) (0.8.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->vllm) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->vllm) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->vllm) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0->vllm) (2024.2.0)\r\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->xgrammar==0.1.11->vllm) (2.0.0)\r\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->xgrammar==0.1.11->vllm) (1.5.0)\r\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->xgrammar==0.1.11->vllm) (2.2.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0->vllm) (2024.2.0)\r\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (13.9.4)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (1.5.4)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.1.2)\r\n",
      "Downloading vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl (264.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading compressed_tensors-0.9.1-py3-none-any.whl (96 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\r\n",
      "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading xgrammar-0.1.11-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.10-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mistral_common-1.5.3-py3-none-any.whl (6.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\r\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.4/376.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\r\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\r\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\r\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\r\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\r\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\n",
      "Downloading rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\r\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: func_timeout\r\n",
      "  Building wheel for func_timeout (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for func_timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15076 sha256=e817f15a08683ba6d499d6e39c267c4e23f29784b328e83924ab1cd610d9fad7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/83/19/b5552bb9630e353f7c5b15be44bf10900afe1abbbfcf536afd\r\n",
      "Successfully built func_timeout\r\n",
      "Installing collected packages: func_timeout, blake3, uvloop, uvicorn, triton, python-multipart, python-dotenv, pycountry, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, lark, jinja2, interegular, httptools, email-validator, diskcache, astor, airportsdata, watchfiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, depyf, rich-toolkit, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, outlines_core, fastapi-cli, transformers, mistral_common, xgrammar, xformers, outlines, gguf, compressed-tensors, vllm\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\r\n",
      "  Attempting uninstall: jinja2\r\n",
      "    Found existing installation: Jinja2 3.1.4\r\n",
      "    Uninstalling Jinja2-3.1.4:\r\n",
      "      Successfully uninstalled Jinja2-3.1.4\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.47.0\r\n",
      "    Uninstalling transformers-4.47.0:\r\n",
      "      Successfully uninstalled transformers-4.47.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "rapids-dask-dependency 24.12.0 requires pynvml<11.5.0a0,>=11.0.0, which is not installed.\r\n",
      "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 24.12.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 24.12.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 blake3-1.0.4 compressed-tensors-0.9.1 depyf-0.18.0 diskcache-5.6.3 email-validator-2.2.0 fastapi-0.115.8 fastapi-cli-0.0.7 func_timeout-4.3.5 gguf-0.10.0 httptools-0.6.4 interegular-0.3.3 jinja2-3.1.5 lark-1.2.2 lm-format-enforcer-0.10.10 mistral_common-1.5.3 msgspec-0.19.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.0.2 pycountry-24.6.1 python-dotenv-1.0.1 python-multipart-0.0.20 rich-toolkit-0.13.2 starlette-0.45.3 torch-2.5.1 transformers-4.49.0 triton-3.1.0 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.7.3 watchfiles-1.0.4 xformers-0.0.28.post3 xgrammar-0.1.11\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y torch pynvml\n",
    "%pip install vllm func_timeout\n",
    "\n",
    "!export VLLM_LOGGING_LEVEL=DEBUG\n",
    "!export VLLM_TRACE_FUNCTION=1\n",
    "\n",
    "!cp -r '/kaggle/input/bird-bench' '/kaggle/working/'\n",
    "# read-only /kaggle/input/ causes issues with opening some databases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3053aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:22.745469Z",
     "iopub.status.busy": "2025-02-24T09:19:22.745163Z",
     "iopub.status.idle": "2025-02-24T09:19:46.356623Z",
     "shell.execute_reply": "2025-02-24T09:19:46.355745Z"
    },
    "papermill": {
     "duration": 23.664064,
     "end_time": "2025-02-24T09:19:46.361057",
     "exception": false,
     "start_time": "2025-02-24T09:19:22.696993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "from enum import Enum\n",
    "from abc import ABC\n",
    "from pathlib import Path\n",
    "from typing import  Sequence, Optional, Literal, Callable\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# from itertools import batched\n",
    "def batched(sequence: Sequence, n: int = 1):\n",
    "    l = len(sequence)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield sequence[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ced857",
   "metadata": {
    "papermill": {
     "duration": 0.047205,
     "end_time": "2025-02-24T09:19:46.460929",
     "exception": false,
     "start_time": "2025-02-24T09:19:46.413724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Database Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a48b801",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:46.577739Z",
     "iopub.status.busy": "2025-02-24T09:19:46.577312Z",
     "iopub.status.idle": "2025-02-24T09:19:46.594607Z",
     "shell.execute_reply": "2025-02-24T09:19:46.593791Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.083624,
     "end_time": "2025-02-24T09:19:46.595889",
     "exception": false,
     "start_time": "2025-02-24T09:19:46.512265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### core/dbhandler.py\n",
    "# import json\n",
    "# import sqlite3\n",
    "# from pathlib import Path\n",
    "# from typing import Optional\n",
    "# from func_timeout import func_timeout, FunctionTimedOut\n",
    "\n",
    "class SQLiteDatabase:\n",
    "    \"\"\" Handler class for sqlite3 databases. Provides SQL execution capabilities and access to schema\"\"\"\n",
    "    def __init__(self, db_id: str, input_path: Path, exec_timeout: float = 30.0, use_cached_schema: Optional[Path] = None) -> None:\n",
    "        \"\"\" Attributes\n",
    "            ----------\n",
    "                db_id: str\n",
    "                    name of database; database must exist in input_path/db_id/db_id.sqlite\n",
    "                input_path: Path\n",
    "                    parent directory of database folder\n",
    "                db_path: Path\n",
    "                    full path of the db_id.sqlite file \n",
    "                exec_timeout: float\n",
    "                    maximum number of seconds for query to return a result; aborts returning [(err),]\n",
    "                schema: dict[str, str]\n",
    "                    either raw_schema or read from json in path \n",
    "\n",
    "                raw_schema: dict[str, str]\n",
    "                    unaugmented, plain db schemas indexed by table_name, read from db_id.sqlite\n",
    "                descriptions: dict[str, str]\n",
    "                    Table descriptions, indexed by table_name, read from table_name.csv \n",
    "                    which exist in input_path/db_id/database_description/\n",
    "\n",
    "                use_cached_schemas: Path | None\n",
    "                    use pre-generated schema stored in path/to/aug.json provided\n",
    "                    instead of raw_schema. File must map db_id: schema.\n",
    "        \"\"\"\n",
    "        self.db_id = db_id\n",
    "        self.input_path = input_path\n",
    "        self.db_path = (self.input_path / self.db_id / self.db_id).with_suffix('.sqlite')\n",
    "        self.exec_timeout = exec_timeout\n",
    "\n",
    "        self.raw_schema: dict[str, str] = self.__fetch_raw_schema()\n",
    "        self.descriptions: dict[str, str] = self.__fetch_db_descriptions()\n",
    "\n",
    "        if use_cached_schema:\n",
    "            with open(use_cached_schema, 'r') as f:\n",
    "                self.schema = json.load(f)[db_id]\n",
    "        else:\n",
    "            self.schema = self.raw_schema\n",
    "\n",
    "    def __getitem__(self, table_name: str):\n",
    "        \"\"\" Return the schema of a table in the database. \"\"\"\n",
    "        return self.schema[table_name] \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\" Returns the database schema as a human-readable/executable string. \"\"\"\n",
    "        return \"\\n\\n\".join( list(self.schema.values()) )\n",
    "    \n",
    "    def run_query(self, sql: str, timeout: Optional[float] = None) -> list[tuple]:\n",
    "        \"\"\" Executes SQL query and fetches all rows. \"\"\"\n",
    "        try:\n",
    "            def execute_sql():\n",
    "                with sqlite3.connect(self.db_path, uri=True) as conn:\n",
    "                    rows = conn.execute(sql).fetchall()\n",
    "                return rows\n",
    "                \n",
    "            rows = func_timeout(timeout=(timeout or self.exec_timeout), func=execute_sql)\n",
    "        except FunctionTimedOut as timeout_error:\n",
    "            rows = [(\"Error: timedout\", )]\n",
    "        return rows\n",
    "    \n",
    "    def __fetch_raw_schema(self) -> dict[str, str]:\n",
    "        \"\"\" Returns a dict of schema of all tables in a .sqlite database indexed by table name \"\"\"\n",
    "        tables = self.run_query(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        schemas: dict[str, str] = {}\n",
    "        for table, in tables:\n",
    "            if table != \"sqlite_sequence\":\n",
    "                schema, = self.run_query(f\"SELECT sql FROM sqlite_master WHERE name='{table}';\")[0]\n",
    "                schemas[table] = schema\n",
    "        return schemas\n",
    "    \n",
    "    def __fetch_db_descriptions(self) -> dict[str, str]:\n",
    "        \"\"\" Returns a dict of database_descriptions from each table_name.csv as strings  \"\"\"\n",
    "        def case_insensitive_file_reader(filepath: Path):\n",
    "            content = f'Descriptions file for table at {filepath} does not exist.'\n",
    "            if filepath.exists():\n",
    "                with open(filepath, 'r', errors='ignore') as file:\n",
    "                    content = file.read()\n",
    "            else:\n",
    "                file_stem = filepath.stem\n",
    "                candidate_stems = [\n",
    "                    stem for stem in (\n",
    "                        file_stem.capitalize(), file_stem.title(), file_stem.upper(), file_stem.lower()\n",
    "                    )\n",
    "                    if filepath.with_stem(stem).exists()\n",
    "                ]\n",
    "                if candidate_stems:\n",
    "                    file_stem = candidate_stems[0]\n",
    "                    filepath = filepath.with_stem(file_stem)\n",
    "                    content = case_insensitive_file_reader(filepath)\n",
    "            return content\n",
    "        \n",
    "        descriptions = {}\n",
    "        for table in self.raw_schema.keys():\n",
    "            filepath = (self.input_path / self.db_id / 'database_description' / table).with_suffix('.csv')\n",
    "            descriptions[table] = case_insensitive_file_reader(filepath)\n",
    "\n",
    "        return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e0ddf",
   "metadata": {
    "papermill": {
     "duration": 0.047035,
     "end_time": "2025-02-24T09:19:46.690182",
     "exception": false,
     "start_time": "2025-02-24T09:19:46.643147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bird-Bench Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a433051",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:46.837221Z",
     "iopub.status.busy": "2025-02-24T09:19:46.836844Z",
     "iopub.status.idle": "2025-02-24T09:19:46.849276Z",
     "shell.execute_reply": "2025-02-24T09:19:46.848460Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.073069,
     "end_time": "2025-02-24T09:19:46.850834",
     "exception": false,
     "start_time": "2025-02-24T09:19:46.777765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### core/birdeval.py\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from core.dbhandler import SQLiteDatabase\n",
    "\n",
    "def get_correctness_labels(df: pd.DataFrame, databases: dict[str, SQLiteDatabase], pred_col: str, true_col: str) -> list[bool]:\n",
    "    ''' Takes DataFrame of BIRD questions with prediction column.\n",
    "        Runs gold and predicted SQL queries on databases given.\n",
    "        Returns labels, where labels[i] is True where pred_sql results same as ground_sql's\n",
    "    '''\n",
    "    labels = []\n",
    "    for i, question in tqdm(df.iterrows(), desc='Executing SQL', total=len(df)):\n",
    "        db = databases[question['db_id']]\n",
    "        try:\n",
    "            pred_res = db.run_query(question[pred_col])\n",
    "            true_res = db.run_query(question[true_col])\n",
    "        except Exception as e:\n",
    "            print(f\"Q_{question['question_id']}: {e.__class__.__name__} {e}\")\n",
    "            labels.append(False)\n",
    "        else:\n",
    "            labels.append( set(pred_res) == set(true_res) )\n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_accuracy(df: pd.DataFrame, pred_col: str, true_col: str, labels: list[bool]) -> str:\n",
    "    ex_report = (\n",
    "        f\"=== EX Results | TrueCol: {true_col} | PredCol: {pred_col} ===\\n\"\n",
    "        f\"Accuracy : {(sum(labels) / len(labels)) * 100: .3f}%\\n\"\n",
    "        \"Breakdown by Difficulty:\\n\"\n",
    "    )    \n",
    "    for difficulty in df['difficulty'].unique():\n",
    "        difficulty_mask = df['difficulty'] == difficulty\n",
    "        correct_rows = [label for label, mask in zip(labels, difficulty_mask) if mask]\n",
    "        n_correct = sum(correct_rows)\n",
    "        n_total = sum(difficulty_mask)\n",
    "        sub_accuracy = (n_correct / n_total) * 100\n",
    "        ex_report += f\"\\t{difficulty}: {sub_accuracy: .3f}% ({n_correct} of {n_total})\\n\"\n",
    "    ex_report += '=== end ===\\n'\n",
    "    return ex_report\n",
    "\n",
    "\n",
    "# # TODO: add soft-f1 score to report\n",
    "def calculate_softf1():\n",
    "    raise NotImplementedError\n",
    "def calculate_ves():\n",
    "    raise NotImplementedError\n",
    "def calculate_rves():\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "def evaluate(df: pd.DataFrame, databases: dict[str, SQLiteDatabase], pred_col: str, true_col: str = 'SQL') -> tuple[list[bool], str]:\n",
    "    print(f'\\n--- Evaluating Performance | TrueCol: {true_col} | PredCol: {pred_col} ---')\n",
    "    labels = get_correctness_labels(df, databases, pred_col, true_col)\n",
    "    ex_report = calculate_accuracy(df, pred_col, true_col, labels)\n",
    "    # f1_report = calculate_softf1(df, pred_col, true_col, labels)\n",
    "    # ves_report = calculate_ves(df, pred_col, true_col, labels)\n",
    "    # rves_report = calculate_rves(df, pred_col, true_col, labels)\n",
    "\n",
    "    # report = \"\\n\\n\".join(ex_report, f1_report, ves_report, rves_report)\n",
    "    report = ex_report      # until the rest gets implemented\n",
    "    print(report)\n",
    "    print(f'--- Evaluation Completed | TrueCol: {true_col} | PredCol: {pred_col} ---\\n')\n",
    "    return labels, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba942c",
   "metadata": {
    "papermill": {
     "duration": 0.047174,
     "end_time": "2025-02-24T09:19:46.952544",
     "exception": false,
     "start_time": "2025-02-24T09:19:46.905370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531181bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:47.049440Z",
     "iopub.status.busy": "2025-02-24T09:19:47.049106Z",
     "iopub.status.idle": "2025-02-24T09:19:47.067020Z",
     "shell.execute_reply": "2025-02-24T09:19:47.066184Z"
    },
    "papermill": {
     "duration": 0.068331,
     "end_time": "2025-02-24T09:19:47.068360",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.000029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import json\n",
    "# import sqlite3\n",
    "# from pathlib import Path\n",
    "# from abc import ABC\n",
    "# from typing import Optional, Callable, Sequence\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from vllm import LLM, SamplingParams\n",
    "# from core.dbhandler import SQLiteDatabase\n",
    "\n",
    "\n",
    "class TextToSQLGenerationOutput:\n",
    "    def __init__(\n",
    "        self, input_prompts: list[str], raw_responses: list[str], parsed_sql: list[str], \n",
    "        n_in_tokens: list[int], n_out_tokens: list[int]\n",
    "    ) -> None:\n",
    "        self.input_prompts = input_prompts\n",
    "        self.raw_responses = raw_responses\n",
    "        self.parsed_sql = parsed_sql\n",
    "        self.n_in_tokens = n_in_tokens\n",
    "        self.n_out_tokens = n_out_tokens\n",
    "\n",
    "    def as_dataframe(self, col_suffix: Optional[str] = '') -> pd.DataFrame:\n",
    "        if col_suffix: \n",
    "            col_suffix = f'_{col_suffix}'\n",
    "        df = pd.DataFrame({\n",
    "            f'input_prompts{col_suffix}': self.input_prompts,\n",
    "            f'raw_responses{col_suffix}': self.raw_responses,\n",
    "            f'parsed_sql{col_suffix}':    self.parsed_sql,\n",
    "            f'n_in_tokens{col_suffix}':   self.n_in_tokens,\n",
    "            f'n_out_tokens{col_suffix}':  self.n_out_tokens,\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def __str__(self):\n",
    "        print(\"Input Prompts: \", self.input_prompts)\n",
    "        print(\"Raw Responses: \", self.raw_responses)\n",
    "        print(\"Parsed SQL: \", self.parsed_sql)\n",
    "        print(\"Total Input Tokens = \", sum(self.n_in_tokens))\n",
    "        print(\"Total Input Tokens = \", sum(self.n_out_tokens))\n",
    "        \n",
    "        \n",
    "class TextToSQL(ABC):\n",
    "    \"\"\" Base class for all Text-to-SQL generation agents. \"\"\"\n",
    "    # TODO: maybe implement a prompt format cleanup function; \n",
    "    # [Does Prompt Formatting Have Any Impact on LLM Performance?](https://arxiv.org/pdf/2411.10541)\n",
    "\n",
    "    def __init__(\n",
    "        self, llm: LLM, databases: dict[str, SQLiteDatabase], \n",
    "        output_path: Path,\n",
    "    ) -> None:\n",
    "        \"\"\" Attributes\n",
    "            ----------\n",
    "                llm: LLM\n",
    "                    Text generation model for offline generation\n",
    "                databases: dict[str, SQLiteDatabase]\n",
    "                    Dictionary of SQLiteDatabases indexed by db_id\n",
    "                output_path: Path\n",
    "                    Directory to dump output json\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.databases = databases\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def process_bird_df(self, idx: int, row: pd.DataFrame, **kwargs) -> tuple:\n",
    "        \"\"\" Takes a row of a DataFrame of BIRD Bench questions. \n",
    "            Processes and returns necessary columns required by this Agent's generate_response(). \n",
    "            Output tuple must be unpackable as parameters to generate_response().\n",
    "        \"\"\"\n",
    "        db = self.databases[ row['db_id'] ]\n",
    "        schema = str(db)\n",
    "        question = f\"{row['question']}  Hint: {row['evidence']}\"\n",
    "        return schema, question\n",
    "\n",
    "    def generate_prompt(self, schema: str, question: str, **kwargs) -> str:\n",
    "        \"\"\" Takes a question and a schema to generate the agent's SQL generation prompt \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def generate_text(self, prompts: list[str], cfg: SamplingParams, use_tqdm: bool = False) -> TextToSQLGenerationOutput:        \n",
    "        outputs = self.llm.generate(prompts, sampling_params=cfg, use_tqdm=use_tqdm)\n",
    "\n",
    "        input_prompts: list[str] = [output.prompt for output in outputs]\n",
    "        raw_responses: list[str] = [output.outputs[0].text for output in outputs]\n",
    "        parsed_sql:    list[str] = [self.auto_parse_sql(response) for response in raw_responses]\n",
    "        n_in_tokens:   list[int] = [len(output.prompt_token_ids) for output in outputs]\n",
    "        n_out_tokens:  list[int] = [len(output.outputs[0].token_ids) for output in outputs]\n",
    "\n",
    "        return TextToSQLGenerationOutput(input_prompts, raw_responses, parsed_sql, n_in_tokens, n_out_tokens)\n",
    "    \n",
    "    def batched_generate(\n",
    "        self, df: pd.DataFrame, cfg: SamplingParams, batch_size: int, \n",
    "        savename: str, evaluator_fn: Optional[Callable] = None, **kwargs\n",
    "    ) -> tuple[TextToSQLGenerationOutput, Optional[list[bool]]]:\n",
    "        \"\"\" Generates SQL from a DataFrame of BIRD questions. Evaluates performance using evaluator_fn.\n",
    "            Saves responses with savename as suffix.\n",
    "            Kwargs passed on to process_bird_df().\n",
    "        \"\"\"\n",
    "        input_prompts: list[str] = []\n",
    "        raw_responses: list[str] = []\n",
    "        parsed_sql:    list[str] = []\n",
    "        n_in_tokens:   list[int] = []\n",
    "        n_out_tokens:  list[int] = []\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(batched(df, batch_size), desc=f'{savename} Generating SQL')):\n",
    "            # Generate\n",
    "            prompts: list[str] = [\n",
    "                self.generate_prompt(*self.process_bird_df(idx, row, **kwargs))\n",
    "                for idx, row in batch.iterrows()\n",
    "            ]\n",
    "            outputs = self.generate_text(prompts, cfg, use_tqdm=False)\n",
    "\n",
    "            # Record responses\n",
    "            input_prompts.extend(outputs.input_prompts)\n",
    "            raw_responses.extend(outputs.raw_responses)\n",
    "            parsed_sql.extend(outputs.parsed_sql)\n",
    "            n_in_tokens.extend(outputs.n_in_tokens)\n",
    "            n_out_tokens.extend(outputs.n_out_tokens)\n",
    "            if savename:\n",
    "                self.dump_to_json(f\"{savename}_raw\", raw_responses)\n",
    "                self.dump_to_json(f\"{savename}_clean\", parsed_sql)\n",
    "        \n",
    "        final_output = TextToSQLGenerationOutput(input_prompts, raw_responses, parsed_sql, n_in_tokens, n_out_tokens)\n",
    "        final_df = pd.concat(\n",
    "            [df, final_output.as_dataframe(col_suffix=savename)], \n",
    "            axis=1,\n",
    "        )\n",
    "        if evaluator_fn:\n",
    "            labels, report = evaluator_fn(final_df, self.databases, f'parsed_sql_{savename}')\n",
    "            final_df[f'label_{savename}'] = labels\n",
    "            with open(self.output_path/f'results_{savename}.txt', 'w') as f:\n",
    "                f.write(report)\n",
    "        else:\n",
    "            labels = None\n",
    "        final_df.to_json(self.output_path / f\"df_batgen_{savename}.json\", orient='records')\n",
    "        return final_output, labels\n",
    "\n",
    "    def parse_with_regex(self, response: str) -> str:\n",
    "        \"\"\" Extracts SQL from responses containing '''sql ... ''' using regex. \"\"\"\n",
    "        try:\n",
    "            sql = re.search(r'```sql(.*?)```', response, re.DOTALL).group(1).strip()\n",
    "        except AttributeError as e:\n",
    "            sql = ''\n",
    "        return sql\n",
    "    \n",
    "    def auto_parse_sql(self, response: str) -> str:\n",
    "        \"\"\" Extracts SQL from responses containing '''sql ... ''' using regex. \n",
    "            If regex search fails, attempts to parse using LLM.\n",
    "            Returns cleaned SQL or an empty string.\n",
    "        \"\"\"\n",
    "        matched = self.parse_with_regex(response)\n",
    "        if not matched:\n",
    "            prompt = (\n",
    "                \"Please extract the SQL query from the text. Enclose your response within \"\n",
    "                \"a ```sql <<your response here>> ``` code block. Exclude any additional \"\n",
    "                \"text from your response, leaving only the SQL.\\n\\n\"\n",
    "                f\"### Text:\\n{response}\\n\\n\"\n",
    "                f\"### SQL:\\n\"\n",
    "            )\n",
    "            raw_output = self.llm.generate(prompt, SamplingParams(temperature=0), use_tqdm=False)\n",
    "            llm_parsed = raw_output[0].outputs[0].text\n",
    "            matched = self.parse_with_regex(llm_parsed)\n",
    "            if matched:\n",
    "                print(\"Successfully parsed with LLM.\")\n",
    "            else:\n",
    "                print(\"Failed to parse with LLM. Returning empty string.\")\n",
    "        return matched\n",
    "    \n",
    "    def is_sql_same(self, db_id: str, query_1: str, query_2: str) -> bool:\n",
    "        \"\"\" Executes SQL queries and returns True if outputs match, with no operation errors. \"\"\"\n",
    "        try:\n",
    "            res_1 = self.databases[db_id].run_query(query_1)\n",
    "            res_2 = self.databases[db_id].run_query(query_2)\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(f\"{e.__class__.__name__} {e}\")\n",
    "            return False\n",
    "        else:\n",
    "            return set(res_1) == set(res_2)\n",
    "        \n",
    "    def dump_to_json(self, filename: str, obj: object) -> None:\n",
    "        \"\"\" Dumps a list of objects to self.output_path/filename.json; use for keeping backups. \"\"\"\n",
    "        filepath = self.output_path / f\"{filename}.json\"\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(obj, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367771d5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:47.166050Z",
     "iopub.status.busy": "2025-02-24T09:19:47.165777Z",
     "iopub.status.idle": "2025-02-24T09:19:47.171920Z",
     "shell.execute_reply": "2025-02-24T09:19:47.171125Z"
    },
    "papermill": {
     "duration": 0.057265,
     "end_time": "2025-02-24T09:19:47.173399",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.116134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sqlgen.base_agent import TextToSQL\n",
    "\n",
    "# TODO: long prompts performs poorly with small models\n",
    "\n",
    "\n",
    "class ZeroShotAgent(TextToSQL):\n",
    "    \"\"\" Zero-shot SQL Generator based on OpenAI Cookbook's \"Natural Language to SQL\" example and zero-shot COT. \"\"\"            \n",
    "    def generate_prompt(self, schema: str, question: str) -> str:\n",
    "        prompt = (\n",
    "            \"Given the following SQLite tables, your job is to write queries given a user’s request. \"\n",
    "            f\"### QUESTION\\n{question}.\\n\\n\"\n",
    "            f\"### SCHEMA\\n{schema}\\n\\n\"\n",
    "            f\"### QUESTION\\n{question}.\\n\\n\"\n",
    "            f\"### RESPONSE\\nLet's think step by step \"\n",
    "        )\n",
    "        return prompt\n",
    "    \n",
    "\n",
    "### DO NOT USE, NEEDS MODIFICATION\n",
    "class MetaPromptZeroShotAgent(ZeroShotAgent):\n",
    "    \"\"\" Zero-shot SQL Generator using detailed meta-prompt of instructions\"\"\"\n",
    "    def generate_prompt(self, schema: str, question: str) -> str:\n",
    "        prompt = (\n",
    "            \"You are an SQLite expert who excels at writing queries. Your job is to write  \"\n",
    "            \"a valid SQLite query to answer a given user question based on the schema below. \"\n",
    "            \"Here is how you should approach the problem:\\n\"\n",
    "            \"1. Begin your response with 'Let\\'s think step by step.'\\n\"\n",
    "            \"2. Analyze the question and schema carefully, showing all your workings:\\n\"\n",
    "            \"   - Decompose the question into subproblems.\\n\"\n",
    "            \"   - Identify the tables and the columns required to write the query.\\n\"\n",
    "            \"   - Identify the operations you will need to perform.\\n\"\n",
    "            \"3. Review your choices before generation:\\n\"\n",
    "            \"   - Identify if you missed any tables and columns.\\n\"\n",
    "            \"   - Identify if you picked any unnecessary tables and columns.\\n\"\n",
    "            \"   - Identify any unnecessary subqueries, joins, groupings, orderings, sortings etc.\\n\"\n",
    "            \"4. Ensure your choices are correct and optimal.\\n\"\n",
    "            \"5. Finally, show your reasoning and write down the SQL query.\\n\\n\"\n",
    "            f\"### Schema:\\n{schema}\\n\\n\"\n",
    "            f\"### Question:\\n{question}.\"\n",
    "        )\n",
    "        return prompt\n",
    "        \n",
    "\n",
    "class OptimizerAgent(TextToSQL):\n",
    "    def process_bird_df(self, idx: int, row: pd.DataFrame, pred_col=str) -> tuple:\n",
    "        schema, question = super().process_bird_df(idx, row)\n",
    "        sql = row[pred_col]\n",
    "        return schema, question, sql\n",
    "\n",
    "    def generate_prompt(self, schema: str, question: str, sql: str) -> str:\n",
    "        prompt = (\n",
    "            \"You are an SQLite expert who excels at debugging and optimizing SQL queries. \"\n",
    "            \"You will be given a database schema, a question, and an SQL query answering \"\n",
    "            \"that question based on the given schema. Carefully analyse the schema, the \"\n",
    "            \"question and the query. Your job is to do the following:\\n\"\n",
    "            \"1. Begin your response with 'Let\\'s think step by step.'\\n\"\n",
    "            \"2. Analyze the query\\n\"\n",
    "            \"   - identify any invalid SQLite keywords.\\n\"\n",
    "            \"   - identify any invalid or missing columns and tables.\\n\"\n",
    "            \"   - identify any unnecessary subqueries, joins, groupings, orderings, sortings etc.\\n\"\n",
    "            \"   - ensure that query is a single SQL statement.\\n\"\n",
    "            \"3. Show your reasoning and write down the corrected, optimized, valid SQLite query.\\n\\n\"\n",
    "            f\"### Schema:\\n{schema}\\n\\n\"\n",
    "            f\"### Question:\\n{question}.\\n\\n\"\n",
    "            f\"### SQL:\\n{sql}\"\n",
    "        )\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2e326b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:47.277230Z",
     "iopub.status.busy": "2025-02-24T09:19:47.276843Z",
     "iopub.status.idle": "2025-02-24T09:19:47.301545Z",
     "shell.execute_reply": "2025-02-24T09:19:47.300707Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.075009,
     "end_time": "2025-02-24T09:19:47.303141",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.228132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from typing import Literal, Callable\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from vllm import LLM, SamplingParams\n",
    "# from sqlgen.base_agent import TextToSQL, TextToSQLGenerationOutput\n",
    "# from core.dbhandler import SQLiteDatabase\n",
    "\n",
    "\n",
    "class ZeroShotStarter(TextToSQL):\n",
    "    personas = {\n",
    "        'simple': \"who offers short, and simple solutions to user questions\",\n",
    "        'technical': \"who provides highly technical answers to user questions\",\n",
    "        'thinker': \"who does not hesistate to dig deep into a problem and explore several approaches before settling on a solution\"\n",
    "    }\n",
    "\n",
    "    def process_bird_df(self, idx: int, row: pd.DataFrame, persona: Literal['simple', 'technical', 'thinker']) -> tuple:\n",
    "        schema, question = super().process_bird_df(idx, row)\n",
    "        return schema, question, self.personas[persona]\n",
    "\n",
    "    def generate_prompt(self, schema: str, question: str, persona: str) -> str:\n",
    "        prompt = (\n",
    "            f\"You are a helpful SQL coding assistant{' ' + persona if persona else ''}. \"\n",
    "            \"Please generate a SQLite query to answer the user question, based on the schema below. \"\n",
    "            \"In your response, first briefly explain your reasoning. Your final answer should be enclosed \"\n",
    "            \"in a markdown code block.\\n\\n\"\n",
    "            f\"### QUESTION\\n{question}.\\n\\n\"\n",
    "            f\"### SCHEMA\\n{schema}\\n\\n\"\n",
    "            f\"### QUESTION\\n{question}.\\n\\n\"\n",
    "            f\"### RESPONSE\\nLet's think step by step \"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class DiscussionAgent(TextToSQL):\n",
    "    # TODO: Add personas to DiscussionAgent\n",
    "    def process_bird_df(self, idx: int, row: pd.DataFrame, agent_responses: list[dict[int, str]]) -> tuple[str, str, dict[int, str]]:\n",
    "        schema, question = super().process_bird_df(idx, row)\n",
    "        return schema, question, agent_responses[idx]\n",
    "    \n",
    "    def generate_prompt(self, schema: str, question: str, agent_responses: dict[int, str]) -> str:\n",
    "        other_responses = ''.join(\n",
    "            f\"###### Agent_{agent}\\n{response}\\n\\n\"\n",
    "            for agent, response in agent_responses.items()\n",
    "        )\n",
    "        prompt = (\n",
    "            \"You are a helpful SQL coding agent. You understand that collaborative discussion is the best way to solve problems. \"\n",
    "            \"Using the other agent's response as additional information, your job is to generate a SQLite query to answer the \"\n",
    "            \"user question based on the schema. In your response, please explain your reasoning clearly so that others may \"\n",
    "            \"give you constructive feedback. Your final answer should be enclosed in a markdown code block.\\n\\n\"\n",
    "            f\"### QUESTION\\n{question}\\n\\n\"\n",
    "            f\"### SCHEMA\\n{schema}\\n\\n\"\n",
    "            f\"### AGENT RESPONSES\\n{other_responses}\"\n",
    "            f\"### QUESTION\\n{question}\\n\\n\"\n",
    "            f\"### YOUR RESPONSE\\nLet's think step by step \"\n",
    "        )\n",
    "        return prompt\n",
    "    \n",
    "\n",
    "class DiscussionJudge(TextToSQL):\n",
    "    def process_bird_df(self, idx: int, row: pd.DataFrame, agent_responses: list[dict[int, str]]) -> tuple:\n",
    "        schema, question = super().process_bird_df(idx, row)\n",
    "        return schema, question, agent_responses[idx]\n",
    "    \n",
    "    def generate_prompt(self, schema: str, question: str, agent_responses: dict[int, str]) -> str:\n",
    "        n_agents = len(agent_responses)\n",
    "        other_responses = ''.join(\n",
    "            f\"###### Agent_{agent}\\n{response}\\n\\n\"\n",
    "            for agent, response in agent_responses.items()\n",
    "        )\n",
    "        prompt = (\n",
    "            f\"You are a SQL expert overseeing {n_agents} coding agents collaborating to answer the user question based on the given schema. \"\n",
    "            f\"Using the other agents' responses as additional information, generate the production-ready SQLite query. \"\n",
    "            f\"Your final answer should be enclosed in a markdown code block.\\n\\n\"\n",
    "            f\"### QUESTION\\n{question}\\n\\n\"\n",
    "            f\"### SCHEMA\\n{schema}\\n\\n\"\n",
    "            f\"### AGENT RESPONSES\\n{other_responses}\"\n",
    "            f\"### QUESTION\\n{question}\\n\\n\"\n",
    "            f\"### VERDICT\\nLet's think step by step \"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class MultiAgentDiscussion:\n",
    "    def discuss(\n",
    "        df: pd.DataFrame, databases: dict[str, SQLiteDatabase], llm: LLM,\n",
    "        output_path: Path, savename: str, batch_size: int, evaluator_fn: Callable\n",
    "    ) -> pd.DataFrame:\n",
    "            \n",
    "        # TODO: Add personas to DiscussionAgent\n",
    "        starter = ZeroShotStarter(llm, databases, output_path)\n",
    "        agent_1 = DiscussionAgent(llm, databases, output_path)\n",
    "        agent_2 = DiscussionAgent(llm, databases, output_path)\n",
    "        agent_3 = DiscussionAgent(llm, databases, output_path)\n",
    "        judge   = DiscussionJudge(llm, databases, output_path)\n",
    "        \n",
    "        cfg = SamplingParams(\n",
    "            temperature=0.6,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.2,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        \n",
    "        def gather_agent_responses(\n",
    "                agent_num1: int, responses_1: TextToSQLGenerationOutput, \n",
    "                agent_num2: int, responses_2: TextToSQLGenerationOutput,\n",
    "            ) -> list[dict[int, str]]:\n",
    "            agent_responses = [\n",
    "                {agent_num1: resp1, agent_num2: resp2}\n",
    "                for resp1, resp2 in zip(responses_1.raw_responses, responses_2.raw_responses)\n",
    "            ]\n",
    "            return agent_responses\n",
    "        \n",
    "        def gather_all_responses(\n",
    "                responses_1: TextToSQLGenerationOutput, \n",
    "                responses_2: TextToSQLGenerationOutput, \n",
    "                responses_3: TextToSQLGenerationOutput,\n",
    "            ) -> list[dict[int, str]]:\n",
    "            agent_responses = [\n",
    "                {1: resp1, 2: resp2, 3: resp3}\n",
    "                for resp1, resp2, resp3 in zip(responses_1.raw_responses, responses_2.raw_responses, responses_3.raw_responses)\n",
    "            ]\n",
    "            return agent_responses\n",
    "        \n",
    "        starters_1, starters_1_label = starter.batched_generate(df, cfg, batch_size, 'starter1', evaluator_fn, persona='simple')\n",
    "        starters_2, starters_2_label = starter.batched_generate(df, cfg, batch_size, 'starter2', evaluator_fn, persona='technical')\n",
    "        starters_3, starters_3_label = starter.batched_generate(df, cfg, batch_size, 'starter3', evaluator_fn, persona='thinker')\n",
    "\n",
    "        agent_1_discuss_r1, a1r1_label = agent_1.batched_generate(df, cfg, batch_size, f'agent1_R{1}', evaluator_fn, agent_responses=gather_agent_responses(2, starters_2, 3, starters_3))\n",
    "        agent_2_discuss_r1, a2r1_label = agent_2.batched_generate(df, cfg, batch_size, f'agent2_R{1}', evaluator_fn, agent_responses=gather_agent_responses(1, starters_1, 3, starters_3))\n",
    "        agent_3_discuss_r1, a3r1_label = agent_3.batched_generate(df, cfg, batch_size, f'agent3_R{1}', evaluator_fn, agent_responses=gather_agent_responses(1, starters_1, 2, starters_2))\n",
    "        \n",
    "        verdict_r1, verdict_r1_label = judge.batched_generate(df, cfg, batch_size, 'judge_r1', evaluator_fn, agent_responses=gather_all_responses(agent_1_discuss_r1, agent_2_discuss_r1, agent_3_discuss_r1))\n",
    "\n",
    "        agent_1_discuss_r2, a1r2_label = agent_1.batched_generate(df, cfg, batch_size, f'agent1_R{2}', evaluator_fn, agent_responses=gather_agent_responses(2, agent_2_discuss_r1, 3, agent_3_discuss_r1))\n",
    "        agent_2_discuss_r2, a2r2_label = agent_2.batched_generate(df, cfg, batch_size, f'agent2_R{2}', evaluator_fn, agent_responses=gather_agent_responses(1, agent_1_discuss_r1, 3, agent_3_discuss_r1))\n",
    "        agent_3_discuss_r2, a3r2_label = agent_3.batched_generate(df, cfg, batch_size, f'agent3_R{2}', evaluator_fn, agent_responses=gather_agent_responses(1, agent_1_discuss_r1, 2, agent_2_discuss_r1))\n",
    "\n",
    "        verdict_r2, verdict_r2_label = judge.batched_generate(df, cfg, batch_size, 'judge_r2', evaluator_fn, agent_responses=gather_all_responses(agent_1_discuss_r2, agent_2_discuss_r2, agent_3_discuss_r2))\n",
    "\n",
    "        agent_1_discuss_r3, a1r3_label = agent_1.batched_generate(df, cfg, batch_size, f'agent1_R{3}', evaluator_fn, agent_responses=gather_agent_responses(2, agent_2_discuss_r2, 3, agent_3_discuss_r2))\n",
    "        agent_2_discuss_r3, a2r3_label = agent_2.batched_generate(df, cfg, batch_size, f'agent2_R{3}', evaluator_fn, agent_responses=gather_agent_responses(1, agent_1_discuss_r2, 3, agent_3_discuss_r2))\n",
    "        agent_3_discuss_r3, a3r3_label = agent_3.batched_generate(df, cfg, batch_size, f'agent3_R{3}', evaluator_fn, agent_responses=gather_agent_responses(1, agent_1_discuss_r2, 2, agent_2_discuss_r2))\n",
    "\n",
    "        verdict_r3, verdict_r3_label = judge.batched_generate(df, cfg, batch_size, 'judge_r3', evaluator_fn, agent_responses=gather_all_responses(agent_1_discuss_r3, agent_2_discuss_r3, agent_3_discuss_r3))\n",
    "\n",
    "        final_df = pd.concat([\n",
    "                df,\n",
    "                starters_1.as_dataframe(col_suffix='start_simple'),\n",
    "                pd.DataFrame({'label_starter1': starters_1_label}),\n",
    "                starters_2.as_dataframe(col_suffix='start_technical'),\n",
    "                pd.DataFrame({'label_starter2': starters_2_label}),\n",
    "                starters_3.as_dataframe(col_suffix='start_thinker'),\n",
    "                pd.DataFrame({'label_starter3': starters_3_label}),\n",
    "\n",
    "                agent_1_discuss_r1.as_dataframe(col_suffix='agent1_r1'),\n",
    "                pd.DataFrame({'label_agent1_R1': a1r1_label}),\n",
    "                agent_2_discuss_r1.as_dataframe(col_suffix='agent2_r1'),\n",
    "                pd.DataFrame({'label_agent2_R1': a2r1_label}),\n",
    "                agent_3_discuss_r1.as_dataframe(col_suffix='agent3_r1'),\n",
    "                pd.DataFrame({'label_agent3_R1': a3r1_label}),\n",
    "\n",
    "                verdict_r1.as_dataframe(col_suffix='judge_r1'),\n",
    "                pd.DataFrame({'label_judge_r1': verdict_r1_label}),\n",
    "\n",
    "                agent_1_discuss_r2.as_dataframe(col_suffix='agent1_r2'),\n",
    "                pd.DataFrame({'label_agent1_R2': a1r2_label}),\n",
    "                agent_2_discuss_r2.as_dataframe(col_suffix='agent2_r2'),\n",
    "                pd.DataFrame({'label_agent2_R2': a2r2_label}),\n",
    "                agent_3_discuss_r2.as_dataframe(col_suffix='agent3_r2'),\n",
    "                pd.DataFrame({'label_agent3_R2': a3r2_label}),\n",
    "\n",
    "                verdict_r2.as_dataframe(col_suffix='judge_r2'),\n",
    "                pd.DataFrame({'label_judge_r2': verdict_r2_label}),\n",
    "\n",
    "                agent_1_discuss_r3.as_dataframe(col_suffix='agent1_r3'),\n",
    "                pd.DataFrame({'label_agent1_R3': a1r3_label}),\n",
    "                agent_2_discuss_r3.as_dataframe(col_suffix='agent2_r3'),\n",
    "                pd.DataFrame({'label_agent2_R3': a2r3_label}),\n",
    "                agent_3_discuss_r3.as_dataframe(col_suffix='agent3_r3'),\n",
    "                pd.DataFrame({'label_agent3_R3': a3r3_label}),\n",
    "\n",
    "                verdict_r3.as_dataframe(col_suffix='judge_r3'),\n",
    "                pd.DataFrame({'label_judge_r3': verdict_r3_label}),\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        final_df.to_json(output_path/f\"df_{savename}_final.json\", orient='records')\n",
    "        print('finished discussion')\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9390fa",
   "metadata": {
    "papermill": {
     "duration": 0.046804,
     "end_time": "2025-02-24T09:19:47.406187",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.359383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b87115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:47.503231Z",
     "iopub.status.busy": "2025-02-24T09:19:47.502957Z",
     "iopub.status.idle": "2025-02-24T09:19:47.507999Z",
     "shell.execute_reply": "2025-02-24T09:19:47.507309Z"
    },
    "papermill": {
     "duration": 0.054329,
     "end_time": "2025-02-24T09:19:47.509171",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.454842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### config.py\n",
    "# from core.llm import SupportedModels\n",
    "# from pathlib import Path\n",
    "\n",
    "class SupoortedModels(Enum):\n",
    "    qwen25_coder_14b_instruct_awq = '/kaggle/input/qwen2.5-coder/transformers/14b-instruct-awq/1'\n",
    "    qwen25_coder_32b_instruct_awq = '/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1'\n",
    "\n",
    "### Experiment Configurations ###\n",
    "### Import only to utils.py   ###\n",
    "EXPERIMENT = [\n",
    "    'zs_32b',\n",
    "    'mad_250-500',\n",
    "    'zs_cot_qscq',\n",
    "    'mad',\n",
    "    'zero-meta-optim-unaug',\n",
    "][0]\n",
    "\n",
    "MODEL = SupoortedModels.qwen25_coder_32b_instruct_awq       # TODO: make this settable from bash somehow maybe?\n",
    "GPU_MEMORY_UTILIZATION = 0.97\n",
    "TENSOR_PARALLEL_SIZE = 2                # set equal to number of GPU               \n",
    "MODEL_MAX_SEQ_LEN = 4096 * 0.5          # 4096*2 is max for 14B AWQ model with fp8 KV-cache on 15GB VRAM \n",
    "KV_CACHE_DTYPE = 'fp8'                  # Reduces memory consumption; fp8 might impact models that use Grouped Query Attn like Qwen\n",
    "BATCH_SIZE = 32                         # saves after every batch\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "INPUT_PATH  = Path(f'/kaggle/working/bird-bench/bird-bench/bird-minidev')\n",
    "OUTPUT_PATH = Path(f'/kaggle/working/results/{MODEL.name}_{EXPERIMENT}/')\n",
    "BIRD_QUESTION_FILENAME = 'dev.json'\n",
    "DATABASES_FOLDERNAME = 'dev_databases'\n",
    "DB_EXEC_TIMEOUT = 30.0                              # maximum number of seconds a query execution is allowed to take\n",
    "USE_CACHED_SCHEMA = None #Path('/kaggle/working/bird-bench/aug-minidev/aug-minidev/aug.json')  # Use pre-generated schema \n",
    "\n",
    "# set all to FALSE for actual runs\n",
    "USE_DEBUG_DATASET = False                          # Debug with only first 5 bird questions\n",
    "USE_DEBUG_DB = False                               # True for ['formula_1', 'debit_card_specializing', 'thrombosis_prediction'] only subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2d796",
   "metadata": {
    "papermill": {
     "duration": 0.047533,
     "end_time": "2025-02-24T09:19:47.604185",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.556652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9483db5",
   "metadata": {
    "papermill": {
     "duration": 0.046866,
     "end_time": "2025-02-24T09:19:47.697891",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.651025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e452b8e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:47.794793Z",
     "iopub.status.busy": "2025-02-24T09:19:47.794432Z",
     "iopub.status.idle": "2025-02-24T09:19:47.799302Z",
     "shell.execute_reply": "2025-02-24T09:19:47.798443Z"
    },
    "papermill": {
     "duration": 0.055291,
     "end_time": "2025-02-24T09:19:47.800499",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.745208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### utils.py\n",
    "\n",
    "### BIRD Dataset Reader Function ###\n",
    "def read_dataset() -> tuple[pd.DataFrame, dict[str, SQLiteDatabase]]:\n",
    "    \"\"\" BIRD dataset reader function.\n",
    "        1. Reads dataset into DataFrame from \"INPUT_PATH/BIRD_QUESTION_FILENAME\".\n",
    "        2. Lists database names from folders in \"INPUT_PATH/DB_FOLDERNAME/\".\n",
    "        3. Creates dict of SQLiteDatabases, indexed by db_name.\n",
    "        Returns DataFrame of BIRD questions and dict of databases.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(INPUT_PATH / BIRD_QUESTION_FILENAME)\n",
    "    db_names: list[str] = [f.name for f in (INPUT_PATH / DATABASES_FOLDERNAME).iterdir()]\n",
    "    databases: dict[str, SQLiteDatabase] = {\n",
    "        db_id: SQLiteDatabase(db_id, (INPUT_PATH / DATABASES_FOLDERNAME), DB_EXEC_TIMEOUT, USE_CACHED_SCHEMA) \n",
    "        for db_id in db_names\n",
    "    }\n",
    "    print(f'{db_names=}, {len(df)=}')\n",
    "    return df, databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90510996",
   "metadata": {
    "papermill": {
     "duration": 0.046297,
     "end_time": "2025-02-24T09:19:47.899694",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.853397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee06ba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:47.993902Z",
     "iopub.status.busy": "2025-02-24T09:19:47.993572Z",
     "iopub.status.idle": "2025-02-24T09:19:47.999768Z",
     "shell.execute_reply": "2025-02-24T09:19:47.998944Z"
    },
    "papermill": {
     "duration": 0.054455,
     "end_time": "2025-02-24T09:19:48.001049",
     "exception": false,
     "start_time": "2025-02-24T09:19:47.946594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_experiment():\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    df, databases = read_dataset()\n",
    "    cfg = SamplingParams(\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.1,\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "    llm = LLM(\n",
    "        MODEL.value,\n",
    "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "        tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "        max_model_len=MODEL_MAX_SEQ_LEN,\n",
    "        max_seq_len_to_capture=MODEL_MAX_SEQ_LEN,\n",
    "        kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    return df, databases, cfg, llm\n",
    "\n",
    "\n",
    "def agent_baseline(\n",
    "    agent: TextToSQL, cfg: SamplingParams, df: pd.DataFrame, \n",
    "    batch_size: int, savename: str, evaluator_fn: Callable, **kwargs\n",
    ") -> tuple[pd.DataFrame, str]:\n",
    "    print(f\"Experiment: {savename}_{'' if USE_CACHED_SCHEMA else 'un'}aug_{MODEL.name}\")\n",
    "    outputs, labels = agent.batched_generate(df, cfg, batch_size, savename, evaluator_fn, **kwargs)\n",
    "\n",
    "    df[f'input_prompts_{savename}'] = outputs.input_prompts\n",
    "    df[f'n_in_tokens_{savename}']   = outputs.n_in_tokens\n",
    "    df[f'raw_responses_{savename}'] = outputs.raw_responses\n",
    "    df[f'n_out_tokens_{savename}']  = outputs.n_out_tokens\n",
    "    df[f'parsed_sql_{savename}']    = outputs.parsed_sql    \n",
    "    df[f'label_{savename}']         = labels\n",
    "    df.to_json(OUTPUT_PATH/f'df_{savename}.json', orient='records')\n",
    "        \n",
    "    print(f\"Experiment: {savename}_{'' if USE_CACHED_SCHEMA else 'un'}aug_{MODEL.name}_{EXPERIMENT} Successfully Completed.\\n\\n\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de54199",
   "metadata": {
    "papermill": {
     "duration": 0.04588,
     "end_time": "2025-02-24T09:19:48.093680",
     "exception": false,
     "start_time": "2025-02-24T09:19:48.047800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8c321",
   "metadata": {
    "papermill": {
     "duration": 0.046594,
     "end_time": "2025-02-24T09:19:48.186419",
     "exception": false,
     "start_time": "2025-02-24T09:19:48.139825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a0aded",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-24T09:19:48.281986Z",
     "iopub.status.busy": "2025-02-24T09:19:48.281677Z",
     "iopub.status.idle": "2025-02-24T09:20:02.019995Z",
     "shell.execute_reply": "2025-02-24T09:20:02.018427Z"
    },
    "papermill": {
     "duration": 13.786436,
     "end_time": "2025-02-24T09:20:02.021267",
     "exception": true,
     "start_time": "2025-02-24T09:19:48.234831",
     "status": "failed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_names=['california_schools', 'student_club', 'card_games', 'codebase_community', 'formula_1', 'toxicology', 'superhero', 'thrombosis_prediction', 'debit_card_specializing', 'european_football_2', 'financial'], len(df)=500\n",
      "INFO 02-24 09:19:48 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 02-24 09:19:59 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'embed', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-24 09:20:01 config.py:628] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 02-24 09:20:01 config.py:1096] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "INFO 02-24 09:20:01 config.py:1382] Defaulting to use mp for distributed inference\n",
      "INFO 02-24 09:20:01 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-24 09:20:01 multiproc_worker_utils.py:300] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-24 09:20:01 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m INFO 02-24 09:20:01 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method init_worker.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242] Traceback (most recent call last):\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2196, in run_method\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 567, in init_worker\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]     self.worker = worker_class(**kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 82, in __init__\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]     self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1048, in __init__\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242]     self.graph_block_tables = np.zeros(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=205)\u001b[0;0m ERROR 02-24 09:20:01 multiproc_worker_utils.py:242] TypeError: 'float' object cannot be interpreted as an integer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-915280229808>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-db543ed3e337>\u001b[0m in \u001b[0;36msetup_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0;31m     llm = LLM(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mgpu_memory_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPU_MEMORY_UTILIZATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                     )\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# to avoid import order issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_engine_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         self.llm_engine = self.engine_class.from_engine_args(\n\u001b[0m\u001b[1;32m    243\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mexecutor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             self.model_config)\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_worker_tasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAwaitable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     def execute_model(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m             )\n\u001b[1;32m    122\u001b[0m             \u001b[0mall_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         self._run_workers(\"load_model\",\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m_run_workers\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    183\u001b[0m         ]\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         driver_worker_output = run_method(self.driver_worker, sent_method,\n\u001b[0m\u001b[1;32m    186\u001b[0m                                           args, kwargs)\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\u001b[0m in \u001b[0;36minit_worker\u001b[0;34m(self, all_kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;31m# To make vLLM config available during worker initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, local_rank, rank, distributed_init_method, is_driver_worker, model_runner_cls)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mModelRunnerClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoderModelRunner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, kv_cache_dtype, is_driver_worker, return_hidden_states, input_registry, mm_registry)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;31m# The shape of the cached block table will be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# (max batch size to capture, max seq len to capture / block size).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         self.graph_block_tables = np.zeros(\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_batchsize_to_capture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_block_per_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             dtype=np.int32)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df, databases, cfg, llm = setup_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebce6f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8de2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:01:31.036144Z",
     "iopub.status.busy": "2025-02-24T09:01:31.035896Z",
     "iopub.status.idle": "2025-02-24T09:02:19.064387Z",
     "shell.execute_reply": "2025-02-24T09:02:19.063591Z",
     "shell.execute_reply.started": "2025-02-24T09:01:31.036122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':       \n",
    "    agent_zs = ZeroShotAgent(llm, databases, OUTPUT_PATH)\n",
    "    df = agent_baseline(agent_zs, cfg, df, BATCH_SIZE, EXPERIMENT, evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c587504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:02:19.065698Z",
     "iopub.status.busy": "2025-02-24T09:02:19.065461Z",
     "iopub.status.idle": "2025-02-24T09:02:19.068994Z",
     "shell.execute_reply": "2025-02-24T09:02:19.068239Z",
     "shell.execute_reply.started": "2025-02-24T09:02:19.065671Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     MultiAgentDiscussion.discuss(\n",
    "#         df=df[250:].reset_index(), \n",
    "#         databases=databases, \n",
    "#         llm=llm, \n",
    "#         output_path=OUTPUT_PATH, \n",
    "#         savename=f'multiag', \n",
    "#         batch_size=BATCH_SIZE, \n",
    "#         evaluator_fn=evaluate, \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb2b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T09:02:19.070154Z",
     "iopub.status.busy": "2025-02-24T09:02:19.069951Z",
     "iopub.status.idle": "2025-02-24T09:02:19.374536Z",
     "shell.execute_reply": "2025-02-24T09:02:19.373499Z",
     "shell.execute_reply.started": "2025-02-24T09:02:19.070135Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r '/kaggle/working/bird-bench'\n",
    "# cleans up the output directory for easy/fast results download"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6566853,
     "sourceId": 10673528,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 161088,
     "modelInstanceId": 138576,
     "sourceId": 162949,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 240.892451,
   "end_time": "2025-02-24T09:20:04.799185",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-24T09:16:03.906734",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
